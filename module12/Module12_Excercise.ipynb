{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Web APIs with Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Status Code: 200\n",
      "Response Content: {'message': 'Data added successfully'}\n"
     ]
    }
   ],
   "source": [
    "### Sending a POST request\n",
    "\n",
    "# Set the API endpoint URL\n",
    "url = 'http://localhost:8001/api/add_data'\n",
    "\n",
    "# Set the request headers\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Set the request data\n",
    "data = {\n",
    "    'Name': 'John Doe',\n",
    "    'Age': 35,\n",
    "    'City': 'New York'\n",
    "}\n",
    "\n",
    "# Send the POST request to the API endpoint\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# Print the response status code and content\n",
    "print('Response Status Code:', response.status_code)\n",
    "print('Response Content:', json.loads(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Status Code: 200\n",
      "[{'Age': 35, 'City': 'New York', 'Name': 'John Doe'}, {'Age': 35, 'City': 'New York', 'Name': 'John Doe'}, {'Age': 35, 'City': 'New York', 'Name': 'John Doe'}, {'Age': 35, 'City': 'New York', 'Name': 'John Doe'}, {'Age': 35, 'City': 'New York', 'Name': 'John Doe'}, {'Age': 35, 'City': 'New York', 'Name': 'John Doe'}]\n"
     ]
    }
   ],
   "source": [
    "### Sending a GET request\n",
    "\n",
    "# Set the API endpoint URL\n",
    "url = 'http://localhost:8001/api/get_data'\n",
    "\n",
    "# Send the GET request to the API endpoint\n",
    "response = requests.get(url)\n",
    "\n",
    "# Print the response status code and content\n",
    "print('Response Status Code:', response.status_code)\n",
    "print(json.loads(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping with Beautiful Soup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = 'https://en.wikipedia.org/wiki/Web_scraping'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-language-alert-in-sidebar-enabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-enabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charse'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page title: Web scraping - Wikipedia\n",
      "Number of links on the page: 386\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the page title\n",
    "title = soup.find('title').text\n",
    "print(f\"Page title: {title}\")\n",
    "\n",
    "# Find all the hyperlinks on the page\n",
    "links = []\n",
    "for link in soup.find_all('a'):\n",
    "    links.append(link.get('href'))\n",
    "print(f\"Number of links on the page: {len(links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.[1] Web scraping software may directly access the World Wide Web using the Hypertext Transfer Protocol or a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p').text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roswell, GA\n",
      "Restaurants\n",
      "Tacos\n",
      "Las Tejitas\n",
      "Taco Bell\n",
      "Tin Lizzyâ€™s Cantina\n",
      "Taco Nest\n",
      "Chipotle Mexican Grill\n",
      "San Pancho Taqueria\n",
      "Rock N Taco\n",
      "El Serranito\n",
      "Fresco Cantina Grille\n",
      "Taco Takeout\n",
      "El Serranito Taqueria\n",
      "A-POLLO\n",
      "Taco Macho\n",
      "La Poblanita Taqueria\n",
      "HOLA! Taqueria & Bar\n",
      "Chronic Tacos\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "\n",
    "# initialize Chrome webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# navigate to Yelp homepage\n",
    "driver.get(\"https://www.yelp.com/\")\n",
    "\n",
    "# find search bar element and input search query\n",
    "search_bar = driver.find_element(By.ID, \"search_description\")\n",
    "search_bar.clear()\n",
    "search_bar.send_keys(\"tacos\")\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "# wait for search results to load\n",
    "sleep(5)\n",
    "\n",
    "# find all search result elements and loop through them\n",
    "results = driver.find_elements(By.CLASS_NAME, \"css-1m051bw\")\n",
    "for result in results:\n",
    "    print(result.text)\n",
    "\n",
    "# close webdriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
